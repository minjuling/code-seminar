{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "**training data를 loading 하기 위한 class**\n",
    "\n",
    "#### dataset에서 하는 일\n",
    "- training data가 저장된 directory에서 `data 읽어 들이기`\n",
    "- train 하기 위한 data로 만들어주기\n",
    "- `data augmentation`\n",
    "\n",
    "#### dataset class function\n",
    "+ __input__:  전체 input feature tensor와 target tensor\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "+ __init__(self): \n",
    "    + 필요한 변수 선언. 전체 x_data와 y_data load하거나 파일목록을 load\n",
    "    + training data가 저장되어 있는 disk location으로부터 training 할 image file 이름을 불러오기\n",
    "    + data augmentation을 위해 필요한 값들 설정  \n",
    "<br>\n",
    "\n",
    "+ __get_item__(self, index): \n",
    "    + idx에 해당하는 training data(torch.tensor)를 return\n",
    "    + 모든 return 값들의 tensor shape이 같아야 함. collate_fn()때문\n",
    "    + 학습 전 preprocessing을 진행\n",
    "    + data augmentation 진행  \n",
    "<br>\n",
    "\n",
    "+ __len__(self): \n",
    "    + len은 training 하기 위한 전체 dataset의 길이를 return\n",
    "    + 여기서 정의된 len에 따라 한 epoch동안 training할 dataset의 길이가 결정 -> mini batch가 만들어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.distributed as dist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision.transforms.functional as TF\n",
    "from skimage import io\n",
    "from torchvision import transforms, utils\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기본 Dataset class\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        super(BasicDataset, self).__init__()\n",
    "\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KITTIDataset(data.Dataset):\n",
    "    def __init__(self, path, mode='train'):\n",
    "        '''\n",
    "        mode = 'train' or 'eval'\n",
    "        '''\n",
    "\n",
    "        super(KITTIDataset, self).__init__()\n",
    "\n",
    "        self.path = path\n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            self.dir_name = 'training'\n",
    "        else:\n",
    "            self.dir_name = 'testing'\n",
    "\n",
    "        self.image_path = os.path.join(self.path,self.dir_name , 'image_2')\n",
    "        self.seg_path = os.path.join(self.path,self.dir_name , 'semantic_rgb')\n",
    "        self.instance_path = os.path.join(self.path,self.dir_name , 'instance')\n",
    "\n",
    "        self.image_files = os.listdir(self.image_path) \n",
    "        # 그 외 학습에 필요한 데이터 정보들...\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def augment(self, image,masks):\n",
    "        \"\"\"\n",
    "        Applying the same augmentation to\n",
    "        image and its corresponding mask\n",
    "        Args:\n",
    "            image(PIL Image): resized image(new_width,new_height)\n",
    "            masks(PIL Image): resized mask(new_width,new_height)\n",
    "        \"\"\"\n",
    "\n",
    "        # Random horizontal flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            masks = TF.hflip(masks)\n",
    "\n",
    "        # Random vertical flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.vflip(image)\n",
    "            masks = TF.vflip(masks)\n",
    "        return image,masks\n",
    "\n",
    "    def transform(self,image,masks,aug):\n",
    "        \"\"\"Applying a set of  transformations as a datapreprocessing task\"\"\"\n",
    "        # convert to PIL Image.\n",
    "        PIL_convert = transforms.ToPILImage()\n",
    "        image = PIL_convert(image)\n",
    "        masks = PIL_convert(masks.astype(np.int32))\n",
    "        # resize the image and masks\n",
    "        resize = transforms.Resize(size=(512,512))\n",
    "        image = resize(image)\n",
    "        masks = resize(masks)\n",
    "        # augmentation\n",
    "        if aug is True:\n",
    "            self.augment(image,masks)\n",
    "        else:\n",
    "            pass\n",
    "        # Convert to Tensor\n",
    "        image = TF.to_tensor(image)\n",
    "        masks = TF.to_tensor(masks)\n",
    "\n",
    "        return image,masks\n",
    "\n",
    "    def __getitem__(self,image_id):\n",
    "        \"\"\"\n",
    "        Function to read the image and mask\n",
    "        and return a sample of dataset when neededself.\n",
    "        Args:\n",
    "        image_id: image index to iterate over the dataset samples\n",
    "        Returns:\n",
    "        sample(dict): a sample of the dataset\n",
    "        \"\"\"\n",
    "        # read the image\n",
    "        image_path  = (os.path.join(self.image_path,self.image_files[image_id]))\n",
    "        image = io.imread(image_path)\n",
    "        # read the mask\n",
    "        # mask_dir = os.path.join(self.seg_path,self.image_files[image_id])\n",
    "        mask_dir = os.path.join(self.seg_path, self.image_files[image_id])\n",
    "        masks_list = []\n",
    "        for i, f in enumerate (next(os.walk(mask_dir))[2]):\n",
    "            if f.endswith ('.png'):\n",
    "                m = io.imread(os.path.join(mask_dir,f)).astype(np.bool)\n",
    "                m = m[:,:,0]\n",
    "                masks_list.append(m)\n",
    "                #combine all the masks corresponding of an invidual sample image into single binary mask\n",
    "                if len(masks_list) != 1:\n",
    "                    masks = np.logical_or(masks,masks_list[i])\n",
    "                else:\n",
    "                    masks = masks_list[i]\n",
    "        # do the transforms..\n",
    "        trans_img,trans_masks = self.transform(image,masks,self.aug)\n",
    "        sample = {\"image\":trans_img,\"masks\":trans_masks}\n",
    "\n",
    "        return(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/93/3_0hffqx21vf1_3c7lplt0g00000gn/T/ipykernel_42703/2605214067.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKITTIDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data_semantics\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/93/3_0hffqx21vf1_3c7lplt0g00000gn/T/ipykernel_42703/530990419.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, image_id)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mmask_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mmasks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = KITTIDataset(\"../data_semantics\")\n",
    "    print(len(dataset))\n",
    "    print(dataset[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "735ca55e8a53331c767e7660433af029be6a86c33db711221777f5c3fb9431ee"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('kaucar': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
